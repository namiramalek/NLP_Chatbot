# -*- coding: utf-8 -*-
"""NamiraMalek_COMP262_assignment1_ex2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xe9Wr_3zt_N3wgU-Zt4zQMhKA-mXduoP
"""

import os
import sys
import random
import numpy as np
import pandas as pd

import json
with open('namira_intents.json') as file:
    data = json.load(file)
    
patterns = []
tags = []
labels = []
responses = []
for i in data['intents']:
    for pattern in i['patterns']:
        patterns.append(pattern)
        tags.append(i['tag'])
    responses.append(i['responses'])
    if i['tag'] not in labels:
        labels.append(i['tag'])

"""Encode the list of intents"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
label_encoder.fit(tags)
training_labels = label_encoder.transform(tags)

"""tokenize the patterns"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
vocabulary_size = 1000
dimension = 16
input_length = 20
tokenizer = Tokenizer(num_words=vocabulary_size) #deal with out of vocab words
tokenizer.fit_on_texts(patterns)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(patterns)
padded_sequences = pad_sequences(sequences, truncating='post', maxlen=input_length) #all sequences into same size

"""Train a deep learning model"""

from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM
from tensorflow.keras.initializers import Constant

model = Sequential()
model.add(Embedding(vocabulary_size, dimension, input_length=input_length))
model.add(GlobalAveragePooling1D())
model.add(Dense(16, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(len(labels), activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""training with 300 """

history = model.fit(padded_sequences, np.array(training_labels), epochs=300)

"""training with 500"""

history = model.fit(padded_sequences, np.array(training_labels), epochs=500)

#save model
model.save("namira_model")
# save tokenizer
import pickle
with open('tokenizer_namira.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
# save encoder
with open('encoder_namira.pickle', 'wb') as ecn_file:
    pickle.dump(label_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)

import random
from tensorflow import keras

# load  model
model = keras.models.load_model('namira_model')
# load tokenizer
with open('tokenizer_namira.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)
# load label encoder 
with open('encoder_namira.pickle', 'rb') as enc:
        label_encoder = pickle.load(enc)
        
while True:
   userInput = input("Enter: ")
   if userInput.lower() == "exit":
     break
   prediction_input = tokenizer.texts_to_sequences([userInput])
   prediction_input = keras.preprocessing.sequence.pad_sequences(prediction_input,truncating='post', maxlen=input_length)
   
   result = model.predict(prediction_input)
   tag = label_encoder.inverse_transform([np.argmax(result)])

   for i in data['intents']:
      if i['tag'] == tag:
         print("Chatbot:" + np.random.choice(i['responses']))
   if tag == "thanks":
        break

